{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c387db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b75306bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_original = xr.open_dataset('chaz-no-random-frozen/2017_001-original-frozen.nc')\n",
    "#ds_vectorized = xr.open_dataset('chaz-no-random-frozen/2017_001-vectorized-frozen.nc')\n",
    "\n",
    "## most recent\n",
    "ds_vectorized = xr.open_dataset('chaz-no-random-frozen/2017_001.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c48363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_xarray_outputs(ds_original, ds_vectorized):\n",
    "    \"\"\"\n",
    "    Compare xarray datasets from original and vectorized versions.\n",
    "    \"\"\"\n",
    "    print(\"=== XARRAY OUTPUT COMPARISON ===\\n\")\n",
    "    \n",
    "    # Extract the Mwspd variable (assuming this is the intensity)\n",
    "    # Shape: (ensembleNum, lifelength, stormID)\n",
    "    mwspd_orig = ds_original['Mwspd'].values\n",
    "    mwspd_vec = ds_vectorized['Mwspd'].values\n",
    "    \n",
    "    print(f\"Shape: {mwspd_orig.shape}\")\n",
    "    print(f\"Dimensions: (ensembleNum={mwspd_orig.shape[0]}, lifelength={mwspd_orig.shape[1]}, stormID={mwspd_orig.shape[2]})\")\n",
    "    \n",
    "    # Get lat/lon for basin identification\n",
    "    lats = ds_original['latitude'].values[0, :]  # First time step\n",
    "    lons = ds_original['longitude'].values[0, :]  # First time step\n",
    "    \n",
    "    # Compare ensemble 0 (deterministic component)\n",
    "    print(\"\\n=== ENSEMBLE 0 (Deterministic) ===\")\n",
    "    mwspd_orig_ens0 = mwspd_orig[0, :, :]  # Shape: (lifelength, stormID)\n",
    "    mwspd_vec_ens0 = mwspd_vec[0, :, :]\n",
    "    \n",
    "    # 1. Check initialization (time=0)\n",
    "    print(\"\\nTime 0 (initialization):\")\n",
    "    diff_init = ~np.isclose(mwspd_orig_ens0[0, :], mwspd_vec_ens0[0, :], equal_nan=True)\n",
    "    n_diff_init = np.sum(diff_init)\n",
    "    print(f\"  Number of differences: {n_diff_init}\")\n",
    "    if n_diff_init > 0:\n",
    "        diff_storms = np.where(diff_init)[0]\n",
    "        print(f\"  First 5 differing storms: {diff_storms[:5]}\")\n",
    "        for iS in diff_storms[:3]:\n",
    "            print(f\"    Storm {iS}: orig={mwspd_orig_ens0[0, iS]:.2f}, vec={mwspd_vec_ens0[0, iS]:.2f}\")\n",
    "            print(f\"      lat={lats[iS]:.2f}, lon={lons[iS]:.2f}\")\n",
    "    \n",
    "    # 2. Check first few time steps\n",
    "    print(\"\\nFirst computed time steps:\")\n",
    "    for t in [2, 4, 6, 8, 10]:\n",
    "        if t < mwspd_orig_ens0.shape[0]:\n",
    "            diff_t = ~np.isclose(mwspd_orig_ens0[t, :], mwspd_vec_ens0[t, :], equal_nan=True)\n",
    "            n_diff = np.sum(diff_t)\n",
    "            print(f\"  Time {t}: {n_diff} differences\")\n",
    "            if n_diff > 0 and t == 2:  # Show details for first difference\n",
    "                diff_storms = np.where(diff_t)[0]\n",
    "                print(f\"    First 3 differing storms: {diff_storms[:3]}\")\n",
    "                for iS in diff_storms[:3]:\n",
    "                    print(f\"      Storm {iS}: orig={mwspd_orig_ens0[t, iS]:.2f}, vec={mwspd_vec_ens0[t, iS]:.2f}, diff={mwspd_orig_ens0[t, iS]-mwspd_vec_ens0[t, iS]:.2f}\")\n",
    "    \n",
    "    # 3. By hemisphere\n",
    "    print(\"\\nBy Hemisphere (across all time):\")\n",
    "    nh_mask = lats >= 0\n",
    "    sh_mask = lats < 0\n",
    "    \n",
    "    # Count total differences across all time steps\n",
    "    diff_all_time = ~np.isclose(mwspd_orig_ens0, mwspd_vec_ens0, equal_nan=True)\n",
    "    nh_diff_total = np.sum(diff_all_time[:, nh_mask])\n",
    "    sh_diff_total = np.sum(diff_all_time[:, sh_mask])\n",
    "    print(f\"  Total NH differences: {nh_diff_total}\")\n",
    "    print(f\"  Total SH differences: {sh_diff_total}\")\n",
    "    \n",
    "    # 4. By basin\n",
    "    print(\"\\nBy Basin (time=2):\")\n",
    "    if mwspd_orig_ens0.shape[0] > 2:\n",
    "        # North Atlantic\n",
    "        na_mask = (lats > 0) & (lons >= -100) & (lons <= 0)\n",
    "        # North Pacific  \n",
    "        np_mask = (lats > 0) & (((lons >= 100) & (lons <= 180)) | ((lons >= -180) & (lons <= -100)))\n",
    "        # NW Pacific\n",
    "        nwp_mask = (lats >= 0) & (lons >= 120) & (lons <= 180)\n",
    "        # Southern\n",
    "        s_mask = lats < 0\n",
    "        \n",
    "        diff_t2 = ~np.isclose(mwspd_orig_ens0[2, :], mwspd_vec_ens0[2, :], equal_nan=True)\n",
    "        print(f\"  N.Atlantic: {np.sum(diff_t2 & na_mask)} differences\")\n",
    "        print(f\"  N.Pacific: {np.sum(diff_t2 & np_mask)} differences\")\n",
    "        print(f\"  NW.Pacific: {np.sum(diff_t2 & nwp_mask)} differences\")\n",
    "        print(f\"  Southern: {np.sum(diff_t2 & s_mask)} differences\")\n",
    "    \n",
    "    # 5. Show detailed time series for first problem storm\n",
    "    print(\"\\n=== DETAILED EXAMPLE ===\")\n",
    "    # Find first storm with differences\n",
    "    diff_any_time = np.any(~np.isclose(mwspd_orig_ens0, mwspd_vec_ens0, equal_nan=True), axis=0)\n",
    "    problem_storms = np.where(diff_any_time)[0]\n",
    "    \n",
    "    if len(problem_storms) > 0:\n",
    "        iS = problem_storms[0]\n",
    "        print(f\"Storm {iS} (first storm with any differences):\")\n",
    "        print(f\"  Location: lat={lats[iS]:.2f}, lon={lons[iS]:.2f}\")\n",
    "        print(f\"  Time series (first 20 steps):\")\n",
    "        print(f\"    Time | Original | Vectorized | Diff\")\n",
    "        for t in range(min(20, mwspd_orig_ens0.shape[0])):\n",
    "            orig_val = mwspd_orig_ens0[t, iS]\n",
    "            vec_val = mwspd_vec_ens0[t, iS]\n",
    "            if not np.isnan(orig_val) and not np.isnan(vec_val):\n",
    "                diff_val = orig_val - vec_val\n",
    "                marker = \" ***\" if abs(diff_val) > 0.01 else \"\"\n",
    "                print(f\"    {t:4d} | {orig_val:8.2f} | {vec_val:8.2f} | {diff_val:8.2f}{marker}\")\n",
    "            else:\n",
    "                print(f\"    {t:4d} | {orig_val:8} | {vec_val:8} | NaN\")\n",
    "    else:\n",
    "        print(\"No differences found!\")\n",
    "    \n",
    "    # 6. Statistical summary\n",
    "    print(\"\\n=== STATISTICAL SUMMARY ===\")\n",
    "    # Only compare non-NaN values\n",
    "    valid_mask = ~np.isnan(mwspd_orig_ens0) & ~np.isnan(mwspd_vec_ens0)\n",
    "    if np.any(valid_mask):\n",
    "        differences = mwspd_orig_ens0[valid_mask] - mwspd_vec_ens0[valid_mask]\n",
    "        print(f\"  Mean difference: {np.mean(differences):.4f}\")\n",
    "        print(f\"  Max absolute difference: {np.max(np.abs(differences)):.4f}\")\n",
    "        print(f\"  Std of differences: {np.std(differences):.4f}\")\n",
    "        print(f\"  Number of non-zero differences: {np.sum(np.abs(differences) > 1e-6)}\")\n",
    "    \n",
    "    return problem_storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fd3e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XARRAY OUTPUT COMPARISON ===\n",
      "\n",
      "Shape: (40, 125, 123)\n",
      "Dimensions: (ensembleNum=40, lifelength=125, stormID=123)\n",
      "\n",
      "=== ENSEMBLE 0 (Deterministic) ===\n",
      "\n",
      "Time 0 (initialization):\n",
      "  Number of differences: 1\n",
      "  First 5 differing storms: [12]\n",
      "    Storm 12: orig=nan, vec=25.00\n",
      "      lat=-12.39, lon=127.87\n",
      "\n",
      "First computed time steps:\n",
      "  Time 2: 2 differences\n",
      "    First 3 differing storms: [12 68]\n",
      "      Storm 12: orig=nan, vec=28.82, diff=nan\n",
      "      Storm 68: orig=45.29, vec=37.33, diff=7.96\n",
      "  Time 4: 7 differences\n",
      "  Time 6: 10 differences\n",
      "  Time 8: 12 differences\n",
      "  Time 10: 12 differences\n",
      "\n",
      "By Hemisphere (across all time):\n",
      "  Total NH differences: 646\n",
      "  Total SH differences: 404\n",
      "\n",
      "By Basin (time=2):\n",
      "  N.Atlantic: 0 differences\n",
      "  N.Pacific: 1 differences\n",
      "  NW.Pacific: 1 differences\n",
      "  Southern: 1 differences\n",
      "\n",
      "=== DETAILED EXAMPLE ===\n",
      "Storm 0 (first storm with any differences):\n",
      "  Location: lat=-8.22, lon=50.11\n",
      "  Time series (first 20 steps):\n",
      "    Time | Original | Vectorized | Diff\n",
      "       0 |    25.00 |    25.00 |     0.00\n",
      "       1 |    29.75 |    29.75 |     0.00\n",
      "       2 |    34.50 |    34.50 |     0.00\n",
      "       3 |    44.81 |    44.81 |     0.00\n",
      "       4 |    55.12 |    55.12 |     0.00\n",
      "       5 |    63.89 |    63.89 |     0.00\n",
      "       6 |    72.67 |    72.67 |     0.00\n",
      "       7 |    59.12 |    59.12 |     0.00\n",
      "       8 |    45.58 |    45.58 |     0.00\n",
      "       9 |    53.74 |    53.74 |     0.00\n",
      "      10 |    61.89 |    61.89 |     0.00\n",
      "      11 |    54.30 |    54.30 |     0.00\n",
      "      12 |    46.71 |    46.71 |     0.00\n",
      "      13 |    58.03 |    58.03 |     0.00\n",
      "      14 |    69.34 |    69.34 |     0.00\n",
      "      15 |    63.14 |    63.14 |     0.00\n",
      "      16 |    56.94 |    56.94 |     0.00\n",
      "      17 |    57.46 |    57.46 |     0.00\n",
      "      18 |    57.98 |    57.98 |     0.00\n",
      "      19 |    58.31 |    54.33 |     3.98 ***\n",
      "\n",
      "=== STATISTICAL SUMMARY ===\n",
      "  Mean difference: 1.1270\n",
      "  Max absolute difference: 90.4666\n",
      "  Std of differences: 10.8889\n",
      "  Number of non-zero differences: 849\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0,   4,   7,  12,  14,  18,  20,  21,  24,  31,  32,  33,  34,\n",
       "        35,  41,  42,  47,  48,  53,  56,  58,  60,  61,  63,  66,  67,\n",
       "        68,  70,  71,  74,  76,  79,  81,  84,  88,  89,  91,  93,  94,\n",
       "        97,  98,  99, 100, 105, 108, 111, 115, 116, 121])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_xarray_outputs(ds_original, ds_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f5afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_E1(bt):\n",
    "    \"\"\"\n",
    "    get errors and residual of errors from predictand bt\n",
    "    return:\n",
    "      errors(E1), \n",
    "      [v0E],\n",
    "      ranges of v0 (cat1)\n",
    "    \"\"\"\n",
    "\n",
    "    NS = np.where((bt['StormYear']>=1981)&(bt['StormYear']<=2012))[0]\n",
    "\n",
    "    data  = bt['errors'][:, NS].values          # shape (T, N)\n",
    "    data2 = bt['StormMwspd'][:, NS].values      # shape (T, N)\n",
    "\n",
    "    data1 = data.ravel(order = 'F')\n",
    "    data4 = data2.ravel(order = 'F')\n",
    "\n",
    "    ## mask out NaNs in data1\n",
    "    nan_mask = ~np.isnan(data1)\n",
    "\n",
    "    E1  = data1[nan_mask]\n",
    "    E0  = data1[nan_mask]      \n",
    "    v0E = data4[nan_mask]\n",
    "\n",
    "    E2 = E0 * E1 * v0E\n",
    "    mask = (E2 == E2)\n",
    "    E0 = E0[mask]\n",
    "    E1 = E1[mask]\n",
    "    v0E = v0E[mask]\n",
    "    c11, c0 = np.polyfit(E0, E1, 1)\n",
    "\n",
    "    vals = v0E\n",
    "    vmin = vals.min()\n",
    "    vmax = vals.max()\n",
    "\n",
    "    ## threshold grids \n",
    "    grid_up = np.arange(vmin, vmax, 10)       # for c1 (min → max)\n",
    "    grid_dn = np.arange(vmax, vmin, -10)      # for c2 (max → min)\n",
    "\n",
    "    ### c1 \n",
    "    ## for every threshold iC in grid_up, compute how many values satisfy v0E < iC\n",
    "    ## (n_vals,1) < (1,n_thresholds)\n",
    "    counts_lt = (vals[:, None] < grid_up[None, :]).sum(axis=0)\n",
    "\n",
    "    ## Pick first threshold where count > 50\n",
    "    idx = np.argmax(counts_lt > 50) if np.any(counts_lt > 50) else len(grid_up)-1\n",
    "    c1 = grid_up[idx]\n",
    "\n",
    "    ### c2 \n",
    "    # for every threshold iC in grid_dn, compute how many values satisfy v0E > iC\n",
    "    counts_gt = (vals[:, None] > grid_dn[None, :]).sum(axis=0)\n",
    "\n",
    "    idx = np.argmax(counts_gt > 50) if np.any(counts_gt > 50) else len(grid_dn)-1\n",
    "    c2 = grid_dn[idx]\n",
    "\n",
    "    ## final range\n",
    "    cat1 = np.arange(c1, c2 + 10, 10)\n",
    "\n",
    "    return E0,v0E,cat1\n",
    "\n",
    "def findError(errort, v0E, v0, cat1):\n",
    "    '''\n",
    "    Calculate error \n",
    "    New version: reduces if statement trees. \n",
    "    '''\n",
    "\n",
    "    if np.isnan(v0):\n",
    "        return 0\n",
    "    \n",
    "    ## find once using searchsorted (faster than find_range)\n",
    "    if v0 <= cat1[0]:\n",
    "        mask = v0E < cat1[1]\n",
    "    elif v0 >= cat1[-1]:\n",
    "        mask = v0E >= cat1[-1]\n",
    "    else:\n",
    "        # Use searchsorted instead of find_range - it's faster\n",
    "        j0 = np.searchsorted(cat1, v0, side='right') - 1\n",
    "        mask = (v0E >= cat1[j0]) & (v0E < cat1[j0+1])\n",
    "    \n",
    "    error1 = errort[mask]\n",
    "    ## FREEZING ERROR\n",
    "    #return rng.choice(error1)\n",
    "    return error1[0]\n",
    "\n",
    "from bisect import bisect_left, bisect_right\n",
    "\n",
    "def find_range(array, a):\n",
    "    \"\"\"\n",
    "    Return two indices in which a is in between in an array\n",
    "    array has to be a monotonic increase or decrease array\n",
    "    \"\"\"\n",
    "    if (np.diff(array)<0).any():\n",
    "        start = bisect_right(array[::-1],a)\n",
    "        end = bisect_left(array[::-1],a)\n",
    "        end = array.shape[0]-start\n",
    "        start = end\n",
    "    else:\n",
    "        start = bisect_right(array,a)\n",
    "        end = bisect_left(array,a)\n",
    "    return (start-1, end)\n",
    "\n",
    "## trying to numba speedup getPrediction\n",
    "## doesn't improve efficiency very much, but doesn't slow it down, still tinkering\n",
    "\n",
    "@numba.jit(nopython=True, cache=True)\n",
    "def _compute_shrd(UShear, VShear):\n",
    "    \"\"\"Compute shear magnitude\"\"\"\n",
    "    return np.sqrt(UShear**2 + VShear**2)\n",
    "\n",
    "@numba.jit(nopython=True, cache=True)\n",
    "def _normalize_and_accumulate(rvar, meanX_val, stdX_val, result_val, dy):\n",
    "    \"\"\"Normalize variable and add to accumulator\"\"\"\n",
    "    rvar_norm = (rvar - meanX_val) / stdX_val\n",
    "    return dy + rvar_norm * result_val\n",
    "\n",
    "def getPrediction_v0input_result(bt,meanX,meanY,stdX,stdY,it,\n",
    "                                 index,fstperiod,result,predictors,\n",
    "                                 TimeDepends,v0,dvdt):\n",
    "    v1 = []\n",
    "    h1 = []\n",
    "    v1.append(v0)\n",
    "    h1.append(bt.Time[it,index])\n",
    "    \n",
    "    for ih in fstperiod:\n",
    "        h1.append(bt.Time[it,index]+timedelta(hours = ih))\n",
    "        lT = int(ih/6)\n",
    "        dy = result[0]\n",
    "        count2 = 0\n",
    "        \n",
    "        ## reduces number of nanmean calls slightly \n",
    "        piwspd_mean = nanmean(bt.PIwspdMean[it:it+lT+1,index])\n",
    "\n",
    "        for var in predictors:\n",
    "            #print meanX.shape\n",
    "            ## change to == instead of 'in' for robustness\n",
    "            if var == 'SHRD':\n",
    "                ## use numba optimized function for SHRD calculation\n",
    "                ushear = bt.UShearMean[it:it+lT+1,index]\n",
    "                vshear = bt.VShearMean[it:it+lT+1,index]\n",
    "                rvar = nanmean(_compute_shrd(ushear, vshear))\n",
    "                #rvar = _nanmean(_compute_shrd(ushear, vshear))\n",
    "            elif var == 'dPIwspd':\n",
    "                #rvar = nanmean(bt.PIwspdMean[it:it+lT+1,index])-v0\n",
    "                #rvar = _nanmean(bt.PIwspdMean[it:it+lT+1,index])-v0\n",
    "                rvar = piwspd_mean-v0\n",
    "            elif var in TimeDepends:\n",
    "                rvar = nanmean(getattr(bt,var)[it:it+lT+1,index])\n",
    "                #rvar = _nanmean(getattr(bt,var)[it:it+lT+1,index])\n",
    "            elif var == 'StormMwspd':\n",
    "                rvar = v0\n",
    "            elif var == 'dVdt':\n",
    "                rvar = dvdt\n",
    "            elif var == 'landmaskMean':\n",
    "                rvar = getattr(bt,var)[it+lT+1,index]-getattr(bt,var)[it,index]\n",
    "            elif var == 'StormMwspd2':\n",
    "                rvar = (getattr(bt,'StormMwspd')[it,index])**2\n",
    "            elif var == 'StormMwspd3':\n",
    "                rvar = (getattr(bt,'StormMwspd')[it,index])**3\n",
    "            elif var == 'MPI':\n",
    "                rvar = piwspd_mean\n",
    "            elif var == 'MPI2':\n",
    "                rvar = piwspd_mean**2\n",
    "            elif var == 'dPIwspd2':\n",
    "                rvar = (piwspd_mean-v0)**2\n",
    "            elif var == 'dPIwspd3':\n",
    "                rvar = (piwspd_mean-v0)**3\n",
    "            elif var == 'dVdt2':\n",
    "                rvar = dvdt**2\n",
    "            elif var == 'dVdt3':\n",
    "                rvar = dvdt**3\n",
    "            else:\n",
    "                rvar = getattr(bt,var)[it,index]\n",
    "\n",
    "            ## normalize and accumulate using numba\n",
    "            dy = _normalize_and_accumulate(rvar, meanX[count2+1], \n",
    "                                           stdX[count2+1], \n",
    "                                           result[count2+1], dy)\n",
    "            count2 = count2 + 1\n",
    "            \n",
    "        dy = (dy*stdY)+meanY\n",
    "        v1.append(v0+dy)\n",
    "    return h1,v1\n",
    "\n",
    "def get_landmask_value(bt, it, lT, iS):\n",
    "    '''\n",
    "    created this to remove duplicate code in get_determin and get_stochastic\n",
    "    '''\n",
    "\n",
    "    # using a single lookup instead of two\n",
    "    ## slightly faster without optimized dask\n",
    "    ## not sure about all edge cases \n",
    "    landmask_it = bt.landmaskMean[it,iS]\n",
    "    landmask_itlT = bt.landmaskMean[it+lT,iS]\n",
    "    \n",
    "    # further simplifying boolean logic\n",
    "    if landmask_itlT <= -0.5 and landmask_it <= -0.5:\n",
    "        return result_w, meanX_w, meanY_w, stdX_w, stdY_w, \\\n",
    "               ['StormMwspd','dVdt','trSpeed','dPIwspd','SHRD','rhMean','dPIwspd2','dPIwspd3','dVdt2']\n",
    "    \n",
    "    ## assuming no other combination available except\n",
    "    ## landmask_itlT > -0.5 and landmask_it > -0.5\n",
    "    else:\n",
    "        return result_l, meanX_l, meanY_l, stdX_l, stdY_l, \\\n",
    "               ['StormMwspd','dVdt','trSpeed','dPIwspd','SHRD','rhMean','dPIwspd2','dPIwspd3','dVdt2','landmaskMean']\n",
    "\n",
    "\n",
    "def get_determin(iiS, block_id=None):\n",
    "    '''\n",
    "    This function finds the deterministic component of the autoregressive TC intensity model.\n",
    "    iiS: bt data for intensity model\n",
    "\n",
    "    '''\n",
    "    iS = np.int_(iiS.mean(keepdims=True))\n",
    "    \n",
    "    lT = 2\n",
    "    \n",
    "    ## pre-extract storm's data once\n",
    "    ## doesn't save that much time, but a little more readable\n",
    "    StormLon_iS = bt.StormLon[:,iS]\n",
    "    StormLat_iS = bt.StormLat[:,iS]\n",
    "    landmaskMean_iS = bt.landmaskMean[:,iS]\n",
    "    \n",
    "    dummy = StormLon_iS[StormLon_iS==StormLon_iS]\n",
    "    TimeDepends = ['dThetaEsMean','T200Mean','rhMean','rh500_300Mean','div200Mean']\n",
    "    \n",
    "    if ((dummy.any()) and (np.abs(StormLat_iS[0])>=5.)):\n",
    "        it1 = 0\n",
    "        it2 = np.int_(np.min([np.argwhere(StormLon_iS==dummy[-1])[-1,0],bt.StormLon.shape[0]-5]))\n",
    "        \n",
    "        if ((StormLat_iS[0] >=0) and (bt.StormLon[0,iS]>=120) and (bt.StormLon[0,iS]<=180)):\n",
    "            #bt.determin[0,iS] = np.max([20, rng.choice(intV[intV==intV])])\n",
    "\n",
    "            ## FREEZING RANDOM\n",
    "            bt.determin[0, iS] = np.max([20, intV[intV==intV][0]])\n",
    "\n",
    "        else:\n",
    "            #bt.determin[0,iS] = np.max([25, rng.choice(intV[intV==intV])])\n",
    "\n",
    "            ## FREEZING RANDOM\n",
    "            bt.determin[0, iS] = np.max([25, intV[intV==intV][1]])\n",
    "        \n",
    "        dvdt = 0.0\n",
    "        ih = 12\n",
    "        lT = np.int_(ih/6)\n",
    "        v0 = bt.determin[0,iS]\n",
    "        \n",
    "        ## made the if statements slightly more legible\n",
    "        for it in range(it1,it2+2,2):\n",
    "            if (\n",
    "                it + lT < bt.StormLon.shape[0]\n",
    "                and not np.isnan(StormLon_iS[it])\n",
    "                and not np.isnan(landmaskMean_iS[it])\n",
    "                and not np.isnan(landmaskMean_iS[it + lT])\n",
    "                and not np.isnan(v0)\n",
    "            ):\n",
    "                ## code was identical between get_determin and get_stochastic, \n",
    "                ## defined a new function \n",
    "                ## may also help with landmask adjustment later\n",
    "                result, meanX, meanY, stdX, stdY, predictors = get_landmask_value(bt, it, lT, iS)\n",
    "                \n",
    "                h1,v1 = getPrediction_v0input_result\\\n",
    "                         (bt,meanX,meanY,stdX,stdY,it,\\\n",
    "                         iS,[ih],result,predictors,\\\n",
    "                         TimeDepends,v0,dvdt)\n",
    "                \n",
    "                v1 = v1[1]\n",
    "                if v1 < 10:\n",
    "                    v1 = np.float64('Nan')\n",
    "                    break\n",
    "                bt.determin[it+lT,iS] = v1\n",
    "                if ((bt.determin[it1:it+lT:lT,iS].max()>35) and (bt.determin[it,iS]<=35) and (bt.determin[it-lT,iS]<=35)):\n",
    "                    break\n",
    "                dvdt = v1-v0\n",
    "                v0 = v1\n",
    "\n",
    "        ## vectorized\n",
    "        iit_indices = np.arange(it1+1, min(it2+1, bt.StormLon.shape[0]-2), 2)\n",
    "        prev_vals = bt.determin[iit_indices-1, iS]\n",
    "        next_vals = bt.determin[iit_indices+1, iS]\n",
    "\n",
    "        mask = ~np.isnan(prev_vals * next_vals)  # true where neither is NaN\n",
    "\n",
    "        bt.determin[iit_indices[mask], iS] = 0.5 * (prev_vals[mask] + next_vals[mask])\n",
    "    \n",
    "    return iS\n",
    "\n",
    "def get_stochastic(iiS,block_id=None):\n",
    "    '''\n",
    "    This function finds the stochastic component of the autoregressive TC intensity model.\n",
    "    iiS: bt data for intensity model\n",
    "    '''\n",
    "    TimeDepends = ['dThetaEsMean','T200Mean','rhMean','rh500_300Mean','div200Mean']\n",
    "    iS = np.int_(iiS.mean(keepdims=True))\n",
    "    dummy = bt.StormLon[:,iS][bt.StormLon[:,iS]==bt.StormLon[:,iS]]\n",
    "    if ((dummy.any()) and (np.abs(bt.StormLat[0,iS])>=5.)):\n",
    "        it1 = 0\n",
    "        it2 = np.min([np.argwhere(bt.StormLon[:,iS]==dummy[-1])[-1,0],bt.StormLon.shape[0]-5])\n",
    "        \n",
    "        if ((bt.StormLat[0,iS] >=0) and (bt.StormLon[0,iS]>=120) and (bt.StormLon[0,iS]<=180)):\n",
    "            ## update to use set rng\n",
    "            #bt.stochastic[0, iS, iNN] = np.max([20, rng.choice(intV[intV==intV])]) #kts\n",
    "            \n",
    "            ## FREEZING RANDOM\n",
    "            bt.stochastic[0, iS, iNN] = np.max([20, intV[intV==intV][0]])\n",
    "\n",
    "        else:\n",
    "            #bt.stochastic[0, iS, iNN] = np.max([25, rng.choice(intV[intV==intV])]) #kts\n",
    "\n",
    "            ## FREEZING RANDOM\n",
    "            bt.stochastic[0, iS, iNN] = np.max([25, intV[intV==intV][1]])\n",
    "            \n",
    "        ih = 12\n",
    "        lT = np.int_(ih/6)\n",
    "        v0 = bt.stochastic[0, iS, iNN]\n",
    "        dvdt = 0.\n",
    "        for it in range(it1,it2+2,2):\n",
    "            ## making if statement a bit more readable\n",
    "            if (\n",
    "                it + lT < bt.StormLon.shape[0]\n",
    "                and not np.isnan(bt.StormLon[it, iS])\n",
    "                and not np.isnan(bt.landmaskMean[it, iS])\n",
    "                and not np.isnan(bt.landmaskMean[it + lT, iS])\n",
    "                and not np.isnan(v0)\n",
    "            ):\n",
    "                \n",
    "                result, meanX, meanY, stdX, stdY, predictors = get_landmask_value(bt, it, lT, iS)\n",
    "   \n",
    "                h1,v1 = getPrediction_v0input_result\\\n",
    "                     (bt,meanX,meanY,stdX,stdY,it,\\\n",
    "                     iS,[ih],result,predictors,\\\n",
    "                     TimeDepends,v0,dvdt)\n",
    "                \n",
    "                error = findError(E0,v0E,v0,cat1)\n",
    "                bt.error[it,iS,iNN] = error\n",
    "                v1 = v1[1]\n",
    "                v1 = v1-error\n",
    "                if v1 < 10:\n",
    "                    break;\n",
    "                \n",
    "                #if ((it >= it1+4*lT) and (bt.stochastic[it,iS,iNN]<=35) and (bt.stochastic[it-lT,iS,iNN]<=35)):\n",
    "                bt.stochastic[it+lT,iS,iNN] = v1\n",
    "                \n",
    "                if ((bt.stochastic[it1:it+lT:lT,iS,iNN].max()>35) and (bt.stochastic[it,iS,iNN]<=35) and (bt.stochastic[it-lT,iS,iNN]<=35)):\n",
    "                    break;\n",
    "                dvdt = v1-v0\n",
    "                v0 = v1\n",
    "\n",
    "\n",
    "        ## vectorized\n",
    "        # iit indices (every 2 steps)\n",
    "        iit_indices = np.arange(it1+1, min(it2+1, bt.StormLon.shape[0]-2), 2)\n",
    "        # neighbor values\n",
    "        prev_vals = bt.stochastic[iit_indices - 1, iS, iNN]\n",
    "        next_vals = bt.stochastic[iit_indices + 1, iS, iNN]\n",
    "        # mask where neither neighbor is NaN\n",
    "        mask = ~np.isnan(prev_vals * next_vals)\n",
    "        # assign averaged values where valid\n",
    "        bt.stochastic[iit_indices[mask], iS, iNN] = 0.5 * (prev_vals[mask] + next_vals[mask])\n",
    "\n",
    "    return iS\n",
    "\n",
    "def load_nc_via_pooch(filename):\n",
    "    \"\"\"Load NetCDF file via pooch and return xarray Dataset.\"\"\"\n",
    "    path = pooch.retrieve(url=f\"{path_data}/{filename}\", known_hash=None)\n",
    "    return xr.open_dataset(path)\n",
    "\n",
    "\n",
    "def calIntensity(iy, ichaz):\n",
    "    '''\n",
    "    This function calculates the intesity using an autoregressive model (Lee et al. (2015, 2016a)).\n",
    "    EXP: location of historical simulations defined in Namelist.py\n",
    "    iy: year of current iteration in CHAZ.py\n",
    "    ichaz: current ensemble iteration in CHAZ.py`\n",
    "    '''\n",
    "\n",
    "    ## load datasets \n",
    "    bt2 = load_nc_via_pooch(gv.obpath)\n",
    "    observed_data_ds = load_nc_via_pooch(\"observed_data.nc\")\n",
    "    coefficient_meanstd_ds = load_nc_via_pooch(\"coefficient_meanstd.nc\")\n",
    "    result_w_ds = load_nc_via_pooch(\"result_w.nc\")\n",
    "    result_l_ds = load_nc_via_pooch(\"result_l.nc\")\n",
    "\n",
    "    global result_w, result_l, meanX_w, meanY_w, stdX_w\n",
    "    global stdY_w, meanX_l, meanY_l, stdX_l, stdY_l\n",
    "    global E0, v0E, cat1, intV\n",
    "\n",
    "    ## convert all variables into numpy arrays \n",
    "    result_w = result_w_ds['params'].values\n",
    "    result_l = result_l_ds['params'].values\n",
    "    meanX_w = coefficient_meanstd_ds.meanX_w.values\n",
    "    meanY_w = coefficient_meanstd_ds.meanY_w.values\n",
    "    stdX_w  = coefficient_meanstd_ds.stdX_w.values\n",
    "    stdY_w  = coefficient_meanstd_ds.stdY_w.values\n",
    "    meanX_l = coefficient_meanstd_ds.meanX_l.values\n",
    "    meanY_l = coefficient_meanstd_ds.meanY_l.values\n",
    "    stdX_l  = coefficient_meanstd_ds.stdX_l.values\n",
    "    stdY_l  = coefficient_meanstd_ds.stdY_l.values\n",
    "\n",
    "    ## TODO change variable name to not overwrite NS below\n",
    "    NS = np.where((bt2['StormYear']>=1981)&(bt2['StormYear']<=2012))[0]\n",
    "    intV = bt2['StormMwspd'][:][0,NS].values\n",
    "    E0, v0E,cat1, = get_E1(bt2)\n",
    "\n",
    "    ## initialize bt\n",
    "    nsto = gv.CHAZ_Int_ENS\n",
    "    bt.__dict__['determin'] = np.zeros(bt.StormLon.shape)*np.float64('nan')\n",
    "    bt.__dict__['stochastic'] = np.zeros([bt.StormLon.shape[0],bt.StormLon.shape[1],nsto])*np.float64('nan')\n",
    "    bt.__dict__['error'] = np.zeros([bt.StormLon.shape[0],bt.StormLon.shape[1],nsto])*np.float64('nan')\n",
    "\n",
    "    ## TODO change variable name to not overwrite NS above\n",
    "    NS = np.arange(0,bt.StormYear.shape[0],1)\n",
    "    nS = da.from_array(NS.astype(dtype=np.int32),chunks=(1,))\n",
    "\n",
    "    new = da.map_blocks(get_determin,nS,chunks=(1,), dtype=nS.dtype)\n",
    "    n = new.compute(scheduler='synchronous',num_workers=10)\n",
    "    #n = new.compute(scheduler='single-threaded',num_workers=10)\n",
    "    del new\n",
    "\n",
    "    new =da.map_blocks(get_stochastic,nS,chunks=(1,), dtype=nS.dtype)\n",
    "    global iNN\n",
    "    for iNN in range(nsto):\n",
    "        n = new.compute(scheduler='synchronous',num_workers=10)\n",
    "        #n = new.compute(scheduler='single-threaded')\n",
    "    del new\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28996485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b12da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd94f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b6505c7",
   "metadata": {},
   "source": [
    "## Keeping track of function descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde501a1",
   "metadata": {},
   "source": [
    "def getPrediction_v0input_result_vectorized(bt, meanX, meanY, stdX, stdY, it,\n",
    "                                           storm_indices, fstperiod, result, predictors,\n",
    "                                           TimeDepends, v0_array, dvdt_array):\n",
    "    \"\"\"\n",
    "    Processes storms simultaneously\n",
    "    storm_indices: array of storm indices to process (nStorms,)\n",
    "    v0_array: initial velocities (nStorms,)\n",
    "    dvdt_array: velocity changes (nStorms,)\n",
    "    returns: v1_array of shape (nStorms,)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_stochastic_vectorized(iNN):\n",
    "    '''\n",
    "    Fully vectorized version that processes all storms simultaneously at each timestep.\n",
    "    This function finds the stochastic component of the autoregressive TC intensity model.\n",
    "    iNN: ensemble member index\n",
    "    Modifies bt in-place (uses global bt).\n",
    "    '''\n",
    "\n",
    "def findError_vectorized(errort, v0E, v0_array, cat1):\n",
    "    '''\n",
    "    Calculate error for multiple storms\n",
    "    v0_array: shape (nStorms,)\n",
    "    returns: shape (nStorms,)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f04f266",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chaz_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
